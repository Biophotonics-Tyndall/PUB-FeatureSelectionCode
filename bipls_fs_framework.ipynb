{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Backward Interval Partial Least Square (biPLS) Feature Selection (FS) Framework"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import matplotlib.transforms\n",
    "import time\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "# Classification\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change to data directory\n",
    "\n",
    "work_path = os.getcwd()\n",
    "print(work_path)\n",
    "\n",
    "os.chdir('YOUR DATA DIRECTORY')\n",
    "data_path = os.getcwd()\n",
    "print(data_path)\n",
    "os.chdir(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "os.chdir(data_path)\n",
    "\n",
    "# Load dataset - df\n",
    "df = pd.read_csv('YOUR DATA FILE.csv')\n",
    "\n",
    "# Average spectra for each tissue type\n",
    "avg_df_by_y = df.groupby(['target_y']).mean()\n",
    "stdev_df_by_y = df.groupby(['target_y']).std()\n",
    "\n",
    "# Column names to numerical\n",
    "col_wavelengths = df.columns.drop('target_y')\n",
    "col_wavelengths = col_wavelengths.astype(np.float64)\n",
    "print('Number of wavelengths: ', len(col_wavelengths))\n",
    "\n",
    "# Legend labels\n",
    "tissue_types = df['target_y'].unique()\n",
    "tissue_types.sort()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SG Smoothing of the raw data - Gentle smoothing with w/p = 2.5 to get rid of some baseline noise\n",
    "\n",
    "# Features\n",
    "X_raw = df.drop(['target_y'], axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['target_y']\n",
    "\n",
    "# Spectral Smoothing - Savitzkyâ€“Golay (SG) method\n",
    "w = 5\n",
    "p = 2\n",
    "X_smooth = savgol_filter(X_raw, w, polyorder=p, axis=1, deriv=0)\n",
    "\n",
    "# Smoothed dataframe\n",
    "df_smooth = pd.DataFrame(X_smooth, columns = col_wavelengths.astype(\"string\"))\n",
    "df_smooth = pd.concat([df_smooth, y.rename('target_y')], axis=1)\n",
    "\n",
    "# Average spectra for each tissue type\n",
    "avg_df_by_y_smooth = df_smooth.groupby(['target_y']).mean()\n",
    "stdev_df_by_y_smooth = df_smooth.groupby(['target_y']).std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change to save path\n",
    "os.chdir(work_path)\n",
    "os.chdir('YOUR SAVE PATH')\n",
    "save_path = os.getcwd()\n",
    "print(save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to calculate Variable Importance in Projection (VIP) in PLS\n",
    "def vip(model):\n",
    "    t = model.x_scores_\n",
    "    w = model.x_weights_\n",
    "    q = model.y_loadings_\n",
    "    p, h = w.shape\n",
    "    vips = np.zeros((p,))\n",
    "    s = np.diag(t.T @ t @ q.T @ q).reshape(h, -1)\n",
    "    total_s = np.sum(s)\n",
    "    for i in range(p):\n",
    "        weight = np.array([(w[i, j] / np.linalg.norm(w[:, j])) ** 2 for j in range(h)])\n",
    "        vips[i] = np.sqrt(p * (s.T @ weight) / total_s)\n",
    "    return vips"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define PLS regression function with cross validation\n",
    "def plsr_cv(X, y, n_comp, cv):\n",
    "\n",
    "    # Define cross validation\n",
    "    cv = cv\n",
    "\n",
    "    # Define scores as goodness-of-fit measures\n",
    "    pls_y_preds = np.array([])\n",
    "    pls_y_trues = np.array([])\n",
    "    pls_r2_scores = []\n",
    "    pls_mse_scores = []\n",
    "    pls_rmse_scores = []\n",
    "\n",
    "    # Evaluate model and cross validation\n",
    "    count = 0\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "        count+=1\n",
    "        # if count == cv.get_n_splits():\n",
    "        #     print('CV finished')\n",
    "        # Define PLS model\n",
    "        pls_model = PLSRegression(n_components=n_comp)\n",
    "\n",
    "        # Define train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Fit PLS model to data\n",
    "        pls_model.fit(X_train, y_train)\n",
    "        # Predict using the fitted model\n",
    "        y_pred = pls_model.predict(X_test)\n",
    "        pls_y_preds = np.concatenate([pls_y_preds, y_pred], axis=None)\n",
    "        # True y labels\n",
    "        pls_y_trues = np.concatenate([pls_y_trues, y_test], axis=None)\n",
    "\n",
    "        # Calculate regression scores\n",
    "        pls_r2 = r2_score(y_test, y_pred)\n",
    "        pls_mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "        pls_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "        # Store scores\n",
    "        pls_r2_scores.append(pls_r2)\n",
    "        pls_mse_scores.append(pls_mse)\n",
    "        pls_rmse_scores.append(pls_rmse)\n",
    "\n",
    "    return pls_y_preds, pls_y_trues, pls_r2_scores, pls_mse_scores, pls_rmse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plsr_ncomp(X, y, n_comp, cv): # Change this to use RandomizedSearchCV\n",
    "\n",
    "    # define the dictionary with parameters to test\n",
    "    grid_param = {'n_components': (np.arange(n_comp) + 1).tolist()}\n",
    "\n",
    "    # define PLS regression\n",
    "    pls = PLSRegression()\n",
    "    # define RandomizedSearchCV\n",
    "    gs_pls = RandomizedSearchCV(pls, param_distributions=grid_param, scoring='neg_root_mean_squared_error',\n",
    "                                cv=cv, return_train_score=True, n_jobs=-1)\n",
    "    # fit GridSearchCV\n",
    "    gs_pls.fit(X, y)\n",
    "\n",
    "    # Best parameters and scores\n",
    "    best_pls_ncomp = gs_pls.best_params_\n",
    "    rmsecv_min = abs(gs_pls.best_score_)\n",
    "    ind_pls_ncom = gs_pls.best_index_\n",
    "    gs_cv_results = gs_pls.cv_results_\n",
    "\n",
    "    return best_pls_ncomp, rmsecv_min, ind_pls_ncom, gs_cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to set intervals by indexing\n",
    "def interval_indexing(ind_list, interval_len):\n",
    "    return [ind_list[i:i+interval_len] for i in range(0, len(ind_list), interval_len)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to plot wavelength votes after optimization\n",
    "def plot_wv_votes(wv, votes, tissue_1, tissue_2, save_flag):\n",
    "\n",
    "    # Set the figure size\n",
    "    f, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    # Tight layout\n",
    "    f.tight_layout()\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(wv, votes, '--o', linewidth=2, label='Wavelength Votes')\n",
    "\n",
    "    # Set figure object\n",
    "    ax.set_title('Wavelength Votes ' + str(tissue_1) + ' vs. ' + str(tissue_2))\n",
    "    ax.set_xlabel('Wavelength (nm)')\n",
    "    ax.set_ylabel('Votes (A.U.)')\n",
    "    ax.set_xticks(np.arange(350, 1950, 100))\n",
    "    ax.set_xlim([350, 1850])\n",
    "    bbox = matplotlib.transforms.Bbox([[-0.2, -0.2], [15, 8.7]])\n",
    "\n",
    "    # Save figure - condition\n",
    "    if save_flag:\n",
    "        if not os.path.exists(str(tissue_1) + '_' + str(tissue_2) + '_wv_votes.png'):\n",
    "            f.savefig(os.getcwd() + '/' +  str(tissue_1) + '_' + str(tissue_2) + '_wv_votes.png', dpi = 1080, bbox_inches =bbox)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to evaluate clf accuracy using lda - Number of Features is a variable\n",
    "# F1 score calculation can be added here\n",
    "\n",
    "def clf_accuracy_eval(X, y, model, cv):\n",
    "\n",
    "    # Initiate empty lists\n",
    "    clf_accuracy_scores = [] # Calculate classification scores\n",
    "    clf_balanced_scores = [] # Calculate balanced classification scores\n",
    "    #clf_f1_scores = [] # Calculate F1 scores\n",
    "    #count = 0\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # count+=1\n",
    "        # print(count, \" of \",  cv.get_n_splits(), \" CV folds \", end=\"\\r\")\n",
    "\n",
    "        # define model\n",
    "        clf_model = model # model needs to be refined in each loop? so move down into the loop?\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Fit model\n",
    "        clf_model.fit(X_train, y_train)\n",
    "        # Predict using the fitted model\n",
    "        y_predict = clf_model.predict(X_test)\n",
    "        # Calculate classification accuracy score\n",
    "        clf_score = accuracy_score(y_test, y_predict)\n",
    "        # Calculate classification balanced accuracy score\n",
    "        clf_balanced_score = balanced_accuracy_score(y_test, y_predict)\n",
    "        # Calculate classification F-1 score\n",
    "        #clf_f1_score = f1_score(y_test, y_predict, average='binary')\n",
    "        # Store each iteration\n",
    "        clf_accuracy_scores.append(clf_score)\n",
    "        clf_balanced_scores.append(clf_balanced_score)\n",
    "        #clf_f1_scores.append(clf_f1_score)\n",
    "\n",
    "    return clf_accuracy_scores, clf_balanced_scores #, clf_f1_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The OVO Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change to save path\n",
    "os.chdir(save_path)\n",
    "os.chdir('YOUR SAVE PATH - SUBFOLDER')\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# biPLS Framework for the OVO approach starts here:\n",
    "\n",
    "# Define parameters\n",
    "df_dataset = df_smooth\n",
    "tissue_types = tissue_types\n",
    "\n",
    "random_state = 42\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=random_state)\n",
    "\n",
    "# Define binary combinations\n",
    "list_tissue_comb = list(combinations(tissue_types,2))\n",
    "# Save the list of binary pairs\n",
    "if not os.path.exists('order_of_tissue_comb.joblib'):\n",
    "    dump(list_tissue_comb, 'order_of_tissue_comb.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the for loop to calculate for all binary pairs\n",
    "\n",
    "# for i,k in combinations(tissue_types,2): # can separate this to make an individual function\n",
    "#\n",
    "#     # Track timestamp - elapsed time\n",
    "#     start_time = time.time()\n",
    "#\n",
    "#     count += 1\n",
    "\n",
    "# if count == 2:\n",
    "#     print('count = 2; break')\n",
    "#     break\n",
    "\n",
    "# Else for single pairs --->\n",
    "\n",
    "i = 'boneCement'\n",
    "k = 'cortBone'\n",
    "\n",
    "# i = 'boneMarrow'\n",
    "# k = 'cortBone'\n",
    "\n",
    "# Select from the original dataset\n",
    "binary_subset = df_dataset[df_dataset['target_y'].isin([i, k])]\n",
    "\n",
    "# Features\n",
    "X_binary_init = binary_subset.drop(['target_y'], axis=1)\n",
    "# Target\n",
    "y_binary_init = binary_subset['target_y']\n",
    "# Convert to 0 and 1\n",
    "y_binary_init = (y_binary_init == i).astype('uint8')\n",
    "\n",
    "\n",
    "# Split Train Test Validation set\n",
    "X_binary, X_val_all, y_binary, y_val_all = train_test_split(X_binary_init, y_binary_init,\n",
    "                                                            test_size=0.2, stratify=y_binary_init, random_state=random_state)\n",
    "\n",
    "# Save train test sets\n",
    "dump(X_binary, str(i) + '_' + str(k) + '_X_binary.joblib')\n",
    "dump(X_val_all, str(i) + '_' + str(k) + '_X_val_all.joblib')\n",
    "dump(y_binary, str(i) + '_' + str(k) + '_y_binary.joblib')\n",
    "dump(y_val_all, str(i) + '_' + str(k) + '_y_val_all.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('First label: ', i)\n",
    "print('Second label: ',k)\n",
    "\n",
    "# Index of all wavelengths\n",
    "wavelength_ind = np.arange(0, len(col_wavelengths),1)\n",
    "\n",
    "# Define the number of PLS components, random seed for cross validation, and wavelength interval length for optimization\n",
    "pls_ncomp_opt = 30 # total number of 30 pls components to maximize\n",
    "random_state_list = [42, 100124, 26224, 325749, 8260] # from random number generation\n",
    "len_interval = [95, 80, 70, 60, 40, 20] # 16-25 intervals from lit; here is to somewhat match with the FWHM of LEDs\n",
    "\n",
    "# Initiate a numpy array to store votes\n",
    "wv_votes = np.zeros(len(col_wavelengths))\n",
    "\n",
    "# Separator\n",
    "sep = '_'\n",
    "\n",
    "# Total time of execution\n",
    "total_time = []\n",
    "\n",
    "# Initiate dictionaries\n",
    "pls_rmsecv_baseline = {}\n",
    "all_avg_rmsecv = {}\n",
    "all_std_rmsecv = {}\n",
    "all_min_rmsecv = {}\n",
    "all_min_rmsecv_ind = {}\n",
    "\n",
    "# Initiate a dictionary of arrays to count how many times each wavelength has been retained for each binary combination\n",
    "all_wv_votes = {}\n",
    "\n",
    "# Track timestamp - elapsed time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over different random seeds to get different compositions of cross validation\n",
    "for random_cv in random_state_list:\n",
    "\n",
    "    print('--- Random state = ', random_cv, ' ---')\n",
    "\n",
    "    # Initiate nested dictionaries\n",
    "    all_avg_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_std_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_min_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_min_rmsecv_ind['Random State ' + str(random_cv)] = {}\n",
    "\n",
    "    # Cross validation\n",
    "    cv_baseline = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=random_cv)\n",
    "    cv_opt = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=random_cv)\n",
    "\n",
    "    # Calculate the optimal number of PLS components for baseline\n",
    "    pls_ncomp_baseline, _, _, _ = plsr_ncomp(X_binary, y_binary, n_comp=pls_ncomp_opt,\n",
    "                                             cv=10)  # GridSearchCV, cv = 10 always\n",
    "    pls_best_ncomp_baseline = pls_ncomp_baseline['n_components']\n",
    "    print('PLS best n_components baseline: ', pls_best_ncomp_baseline)\n",
    "\n",
    "    # Calculate the PLS rmsecv for cv_baseline\n",
    "    _, _, _, _, pls_rmse_scores = plsr_cv(X_binary, y_binary, n_comp=pls_best_ncomp_baseline, cv=cv_baseline)\n",
    "    rmsecv_basline = np.mean(pls_rmse_scores)\n",
    "    print('PLS Baseline RMSECV: %.5f +- %.5f' % (rmsecv_basline, np.std(pls_rmse_scores)))\n",
    "    pls_rmsecv_baseline['Random State ' + str(random_cv)] = rmsecv_basline\n",
    "\n",
    "    # Loop over different lengths to get different borders between intervals\n",
    "    for length in len_interval:\n",
    "\n",
    "        print('--- Interval length = ', length, ' ---')\n",
    "\n",
    "        # Initiate nested dictionaries\n",
    "        all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "\n",
    "        # Set initial parameters\n",
    "        rmsecv_init = np.copy(rmsecv_basline)\n",
    "        X_binary_opt = X_binary.copy()\n",
    "        ind_intervals_len = interval_indexing(wavelength_ind, length)  # Calculate index intervals based on the length\n",
    "        wavelength_ind_opt = np.copy(wavelength_ind)\n",
    "\n",
    "        print('Number of intervals = ', len(ind_intervals_len))\n",
    "\n",
    "        # Initiate lists to store variables for the inner loop\n",
    "        removed_ind_list_inner = []\n",
    "        rest_ind_list_inner = []\n",
    "        avg_rmse_scores = []\n",
    "        avg_rmse_std = []\n",
    "\n",
    "        for rep in range(len(ind_intervals_len)):\n",
    "\n",
    "            print('--- Interval Rep = ', rep, ' ---')\n",
    "\n",
    "            # Initiate nested dictionaries\n",
    "            all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "\n",
    "            # Initiate lists to store variables for the inner loop\n",
    "            removed_ind_list_inner = []\n",
    "            rest_ind_list_inner = []\n",
    "            avg_rmse_scores = []\n",
    "            avg_rmse_std = []\n",
    "\n",
    "            for wv_n in range(len(ind_intervals_len)):\n",
    "                #print('Wavelength Interval Iteration #: ', wv_n)\n",
    "\n",
    "                # Initiate parameters for the inner loop\n",
    "                ind_intervals_inner = interval_indexing(wavelength_ind_opt, length)\n",
    "                removed_ind_inner = ind_intervals_inner.pop(wv_n)\n",
    "                removed_ind_list_inner.append(removed_ind_inner)\n",
    "                #print('The removed index: ', removed_ind_inner)\n",
    "                rest_ind_inner = np.concatenate(ind_intervals_inner).ravel()\n",
    "                rest_ind_list_inner.append(rest_ind_inner)\n",
    "                #print('The rest indices: ', rest_ind_inner)\n",
    "\n",
    "                # Calculate the best number of PLS components\n",
    "                pls_ncomp_inner, _, _, _ = plsr_ncomp(X_binary_opt.iloc[:, rest_ind_inner], y_binary,\n",
    "                                                      n_comp=pls_ncomp_opt, cv=10)  # GridSearchCV, cv = 10 always\n",
    "                pls_best_ncomp_inner = pls_ncomp_inner['n_components']\n",
    "                #print('PLS best n_components inner: ', pls_best_ncomp_inner)\n",
    "\n",
    "                _, _, _, _, pls_rmse_scores_inner = plsr_cv(X_binary_opt.iloc[:, rest_ind_inner], y_binary,\n",
    "                                                            n_comp=pls_best_ncomp_inner, cv=cv_opt)\n",
    "                avg_rmse_inner = np.mean(pls_rmse_scores_inner)\n",
    "                #print('- biPLS RMSECV inner: %.5f +- %.5f' % (avg_rmse_inner, np.std(pls_rmse_scores_inner)))\n",
    "                print('Iteration: %.0f; biPLS RMSECV inner: %.5f +- %.5f' % (wv_n, avg_rmse_inner, np.std(pls_rmse_scores_inner)))\n",
    "                avg_rmse_scores.append(avg_rmse_inner)\n",
    "                avg_rmse_std.append(np.std(pls_rmse_scores_inner))\n",
    "\n",
    "            # Store all average PLS rmsecv and std for each random state and interval length\n",
    "            all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = avg_rmse_scores\n",
    "            all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = avg_rmse_std\n",
    "\n",
    "            # Calculate and store the minimum rmsecv values and its index\n",
    "            min_rmsecv = np.min(avg_rmse_scores)\n",
    "            min_rmsecv_ind = np.argmin(avg_rmse_scores)\n",
    "            all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = min_rmsecv\n",
    "            all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = min_rmsecv_ind\n",
    "            #print('The nth interval with lowest RMSECV: ', min_rmsecv_ind)\n",
    "\n",
    "            if min_rmsecv < rmsecv_init:\n",
    "                #print('Min RMSECV: ', min_rmsecv)\n",
    "                #print('Initial RMSECV: ', rmsecv_init)\n",
    "                #print('-> Yes smaller rmsecv')\n",
    "                rmsecv_init = min_rmsecv  # set new baseline rmsecv\n",
    "                print('-> New baseline rmsecv updated: ', rmsecv_init)\n",
    "                # print('First wv of the deleted interval: ',\n",
    "                #       X_binary_opt.iloc[:, removed_ind_list_inner[min_rmsecv_ind]].columns.astype(np.float64)[0])\n",
    "                # Delete the wavelength interval whose removal results in the lowest RMSECV\n",
    "                X_binary_opt = X_binary_opt.iloc[:, rest_ind_list_inner[min_rmsecv_ind]]\n",
    "                # Delete the wavelength interval index\n",
    "                removed_ind_opt = ind_intervals_len.pop(min_rmsecv_ind)\n",
    "                # The remaining indices - to get length\n",
    "                wavelength_ind_opt = np.concatenate(ind_intervals_len).ravel()\n",
    "                # Update wavelength_ind for optimization\n",
    "                wavelength_ind_opt = np.linspace(0, len(wavelength_ind_opt) - 1, len(wavelength_ind_opt), dtype=int)\n",
    "                # Find the remained wavelengths\n",
    "                wv_opt = X_binary_opt.columns.astype(np.float64)\n",
    "                print('The number of remained wavelength (inner): ', len(wv_opt))\n",
    "                # Find the index of remained wavelength in the original wavelength array\n",
    "                wv_opt_ind = [idx for idx, value in enumerate(col_wavelengths) if value in wv_opt]\n",
    "                # Count votes\n",
    "                wv_votes[wv_opt_ind] += 1\n",
    "            else:\n",
    "                print('NO ALL GREATER; BREAK')\n",
    "                print('--- End of optimization at Rep = ', rep, ' ---')\n",
    "                break\n",
    "\n",
    "# Store the votes\n",
    "all_wv_votes = wv_votes\n",
    "\n",
    "# Plot wavelength votes\n",
    "plot_wv_votes(col_wavelengths, wv_votes, i, k, save_flag=True)\n",
    "\n",
    "# End timestamp\n",
    "end_time = time.time()\n",
    "print('Time to run for each pair: ', (end_time - start_time) / 60, ' minutes')\n",
    "total_time.append(end_time - start_time)\n",
    "print('\\n')\n",
    "\n",
    "# Save variables\n",
    "dump(wv_votes, str(i) + '_' + str(k) + '_wv_votes.joblib')\n",
    "\n",
    "dump(pls_rmsecv_baseline, str(i) + '_' + str(k) + '_pls_rmsecv_baseline.joblib')\n",
    "dump(all_avg_rmsecv, str(i) + '_' + str(k) + '_all_avg_rmsecv.joblib')\n",
    "dump(all_std_rmsecv, str(i) + '_' + str(k) + '_all_std_rmsecv.joblib')\n",
    "dump(all_min_rmsecv, str(i) + '_' + str(k) + '_all_min_rmsecv.joblib')\n",
    "dump(all_min_rmsecv_ind, str(i) + '_' + str(k) + '_all_min_rmsecv_ind.joblib')\n",
    "dump(total_time, str(i) + '_' + str(k) + '_total_time.joblib')\n",
    "dump(X_binary, str(i) + '_' + str(k) + '_X_binary.joblib')\n",
    "dump(X_val_all, str(i) + '_' + str(k) + '_X_val_all.joblib')\n",
    "dump(y_binary, str(i) + '_' + str(k) + '_y_binary.joblib')\n",
    "dump(y_val_all, str(i) + '_' + str(k) + '_y_val_all.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The OVR Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change to save_path\n",
    "os.chdir(save_path)\n",
    "os.chdir('YOUR SAVE PATH - SUBFOLDER')\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "df_dataset = df_smooth\n",
    "tissue_types = tissue_types\n",
    "\n",
    "# Set global random state\n",
    "random_state = 42\n",
    "\n",
    "# Define cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=random_state)\n",
    "\n",
    "# Bone Cement = 1 vs. [Cortical Bone, Trabecular Bone, Cartilage, Bone Marrow] = 0\n",
    "i = 'boneCement'\n",
    "k_1 = 'cortBone'\n",
    "k_2 = 'traBone'\n",
    "k_3 = 'cartilage'\n",
    "k_4 = 'boneMarrow'\n",
    "\n",
    "# Cortical Bone = 1 vs. [Trabecular Bone, Cartilage, Bone Marrow, Muscle] = 0\n",
    "# i = 'cortBone'\n",
    "# k_1 = 'traBone'\n",
    "# k_2 = 'muscle'\n",
    "# k_3 = 'cartilage'\n",
    "# k_4 = 'boneMarrow'\n",
    "\n",
    "print('First Label: ', i)\n",
    "print('Second Label: ' + k_1 + ', ' + k_2 + ', ' + k_3 + ', ' + k_4)\n",
    "\n",
    "# Data selection for multiple tissue types\n",
    "df_subset_ovr = df_dataset[df_dataset['target_y'].isin([i, k_1, k_2, k_3, k_4])]\n",
    "binary_subset = df_dataset[df_dataset['target_y'].isin([i, k_1, k_2, k_3, k_4])]\n",
    "binary_subset.loc[df['target_y'].isin([k_1, k_2, k_3, k_4]), 'target_y'] = 'rest'\n",
    "binary_subset.loc[df['target_y'].isin([i]), 'target_y'] = i\n",
    "k = 'rest'\n",
    "\n",
    "# Features\n",
    "X_binary_init = binary_subset.drop(['target_y'], axis=1)\n",
    "# Target\n",
    "y_binary_init = binary_subset['target_y']\n",
    "# Convert to 0 and 1 - i = 1, k = 0\n",
    "y_binary_init = (y_binary_init == i).astype('uint8')\n",
    "\n",
    "# Split Train Test Validation set\n",
    "X_binary, X_val_all, y_binary, y_val_all = train_test_split(X_binary_init, y_binary_init,\n",
    "                                                            test_size=0.2, stratify=y_binary_init,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "# Save train test sets\n",
    "dump(X_binary, str(i) + '_' + str(k) + '_X_binary.joblib')\n",
    "dump(X_val_all, str(i) + '_' + str(k) + '_X_val_all.joblib')\n",
    "dump(y_binary, str(i) + '_' + str(k) + '_y_binary.joblib')\n",
    "dump(y_val_all, str(i) + '_' + str(k) + '_y_val_all.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('First Label: ', i)\n",
    "print('Second Label: ' + k_1 + ', ' + k_2 + ', ' + k_3 + ', ' + k_4)\n",
    "\n",
    "# Index of all wavelengths\n",
    "wavelength_ind = np.arange(0, len(col_wavelengths),1)\n",
    "\n",
    "# Define the number of PLS components, random seed for cross validation, and wavelength interval length for optimization\n",
    "pls_ncomp_opt = 30 # total number of 30 pls components to maximize\n",
    "random_state_list = [42, 100124, 26224, 325749, 8260] # from random number generation\n",
    "len_interval = [95, 80, 70, 60, 40, 20] # 16-25 intervals from lit; here is to somewhat match with the FWHM of LEDs\n",
    "\n",
    "# Initiate a numpy array to store votes\n",
    "wv_votes = np.zeros(len(col_wavelengths))\n",
    "\n",
    "# Separator\n",
    "sep = '_'\n",
    "\n",
    "# Total time of execution\n",
    "total_time = []\n",
    "\n",
    "# Initiate dictionaries\n",
    "pls_rmsecv_baseline = {}\n",
    "all_avg_rmsecv = {}\n",
    "all_std_rmsecv = {}\n",
    "all_min_rmsecv = {}\n",
    "all_min_rmsecv_ind = {}\n",
    "\n",
    "# Initiate a dictionary of arrays to count how many times each wavelength has been retained for each binary combination\n",
    "all_wv_votes = {}\n",
    "\n",
    "# Track timestamp - elapsed time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over different random seeds to get different compositions of cross validation\n",
    "for random_cv in random_state_list:\n",
    "\n",
    "    print('--- Random state = ', random_cv, ' ---')\n",
    "\n",
    "    # Initiate nested dictionaries\n",
    "    all_avg_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_std_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_min_rmsecv['Random State ' + str(random_cv)] = {}\n",
    "    all_min_rmsecv_ind['Random State ' + str(random_cv)] = {}\n",
    "\n",
    "    # Cross validation\n",
    "    cv_baseline = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=random_cv)\n",
    "    cv_opt = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=random_cv)\n",
    "\n",
    "    # Calculate the optimal number of PLS components for baseline\n",
    "    pls_ncomp_baseline, _, _, _ = plsr_ncomp(X_binary, y_binary, n_comp=pls_ncomp_opt,\n",
    "                                             cv=10)  # GridSearchCV, cv = 10 always\n",
    "    pls_best_ncomp_baseline = pls_ncomp_baseline['n_components']\n",
    "    print('PLS best n_components baseline: ', pls_best_ncomp_baseline)\n",
    "\n",
    "    # Calculate the PLS rmsecv for cv_baseline\n",
    "    _, _, _, _, pls_rmse_scores = plsr_cv(X_binary, y_binary, n_comp=pls_best_ncomp_baseline, cv=cv_baseline)\n",
    "    rmsecv_basline = np.mean(pls_rmse_scores)\n",
    "    print('PLS Baseline RMSECV: %.5f +- %.5f' % (rmsecv_basline, np.std(pls_rmse_scores)))\n",
    "    pls_rmsecv_baseline['Random State ' + str(random_cv)] = rmsecv_basline\n",
    "\n",
    "    # Loop over different lengths to get different borders between intervals\n",
    "    for length in len_interval:\n",
    "\n",
    "        print('--- Interval length = ', length, ' ---')\n",
    "\n",
    "        # Initiate nested dictionaries\n",
    "        all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "        all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)] = {}\n",
    "\n",
    "        # Set initial parameters\n",
    "        rmsecv_init = np.copy(rmsecv_basline)\n",
    "        X_binary_opt = X_binary.copy()\n",
    "        ind_intervals_len = interval_indexing(wavelength_ind, length)  # Calculate index intervals based on the length\n",
    "        wavelength_ind_opt = np.copy(wavelength_ind)\n",
    "\n",
    "        print('Number of intervals = ', len(ind_intervals_len))\n",
    "\n",
    "        # Initiate lists to store variables for the inner loop\n",
    "        removed_ind_list_inner = []\n",
    "        rest_ind_list_inner = []\n",
    "        avg_rmse_scores = []\n",
    "        avg_rmse_std = []\n",
    "\n",
    "        for rep in range(len(ind_intervals_len)):\n",
    "\n",
    "            print('--- Interval Rep = ', rep, ' ---')\n",
    "\n",
    "            # Initiate nested dictionaries\n",
    "            all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "            all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = {}\n",
    "\n",
    "            # Initiate lists to store variables for the inner loop\n",
    "            removed_ind_list_inner = []\n",
    "            rest_ind_list_inner = []\n",
    "            avg_rmse_scores = []\n",
    "            avg_rmse_std = []\n",
    "\n",
    "            for wv_n in range(len(ind_intervals_len)):\n",
    "                #print('Wavelength Interval Iteration #: ', wv_n)\n",
    "\n",
    "                # Initiate parameters for the inner loop\n",
    "                ind_intervals_inner = interval_indexing(wavelength_ind_opt, length)\n",
    "                removed_ind_inner = ind_intervals_inner.pop(wv_n)\n",
    "                removed_ind_list_inner.append(removed_ind_inner)\n",
    "                #print('The removed index: ', removed_ind_inner)\n",
    "                rest_ind_inner = np.concatenate(ind_intervals_inner).ravel()\n",
    "                rest_ind_list_inner.append(rest_ind_inner)\n",
    "                #print('The rest indices: ', rest_ind_inner)\n",
    "\n",
    "                # Calculate the best number of PLS components\n",
    "                pls_ncomp_inner, _, _, _ = plsr_ncomp(X_binary_opt.iloc[:, rest_ind_inner], y_binary,\n",
    "                                                      n_comp=pls_ncomp_opt, cv=10)  # GridSearchCV, cv = 10 always\n",
    "                pls_best_ncomp_inner = pls_ncomp_inner['n_components']\n",
    "                #print('PLS best n_components inner: ', pls_best_ncomp_inner)\n",
    "\n",
    "                _, _, _, _, pls_rmse_scores_inner = plsr_cv(X_binary_opt.iloc[:, rest_ind_inner], y_binary,\n",
    "                                                            n_comp=pls_best_ncomp_inner, cv=cv_opt)\n",
    "                avg_rmse_inner = np.mean(pls_rmse_scores_inner)\n",
    "                #print('- biPLS RMSECV inner: %.5f +- %.5f' % (avg_rmse_inner, np.std(pls_rmse_scores_inner)))\n",
    "                print('Iteration: %.0f; biPLS RMSECV inner: %.5f +- %.5f' % (wv_n, avg_rmse_inner, np.std(pls_rmse_scores_inner)))\n",
    "                avg_rmse_scores.append(avg_rmse_inner)\n",
    "                avg_rmse_std.append(np.std(pls_rmse_scores_inner))\n",
    "\n",
    "            # Store all average PLS rmsecv and std for each random state and interval length\n",
    "            all_avg_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = avg_rmse_scores\n",
    "            all_std_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = avg_rmse_std\n",
    "\n",
    "            # Calculate and store the minimum rmsecv values and its index\n",
    "            min_rmsecv = np.min(avg_rmse_scores)\n",
    "            min_rmsecv_ind = np.argmin(avg_rmse_scores)\n",
    "            all_min_rmsecv['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = min_rmsecv\n",
    "            all_min_rmsecv_ind['Random State ' + str(random_cv)]['Interval ' + str(length)]['Rep ' + str(rep)] = min_rmsecv_ind\n",
    "            #print('The nth interval with lowest RMSECV: ', min_rmsecv_ind)\n",
    "\n",
    "            if min_rmsecv < rmsecv_init:\n",
    "                #print('Min RMSECV: ', min_rmsecv)\n",
    "                #print('Initial RMSECV: ', rmsecv_init)\n",
    "                #print('-> Yes smaller rmsecv')\n",
    "                rmsecv_init = min_rmsecv  # set new baseline rmsecv\n",
    "                print('-> New baseline rmsecv updated: ', rmsecv_init)\n",
    "                # print('First wv of the deleted interval: ',\n",
    "                #       X_binary_opt.iloc[:, removed_ind_list_inner[min_rmsecv_ind]].columns.astype(np.float64)[0])\n",
    "                # Delete the wavelength interval whose removal results in the lowest RMSECV\n",
    "                X_binary_opt = X_binary_opt.iloc[:, rest_ind_list_inner[min_rmsecv_ind]]\n",
    "                # Delete the wavelength interval index\n",
    "                removed_ind_opt = ind_intervals_len.pop(min_rmsecv_ind)\n",
    "                # The remaining indices - to get length\n",
    "                wavelength_ind_opt = np.concatenate(ind_intervals_len).ravel()\n",
    "                # Update wavelength_ind for optimization\n",
    "                wavelength_ind_opt = np.linspace(0, len(wavelength_ind_opt) - 1, len(wavelength_ind_opt), dtype=int)\n",
    "                # Find the remained wavelengths\n",
    "                wv_opt = X_binary_opt.columns.astype(np.float64)\n",
    "                print('The number of remained wavelength (inner): ', len(wv_opt))\n",
    "                # Find the index of remained wavelength in the original wavelength array\n",
    "                wv_opt_ind = [idx for idx, value in enumerate(col_wavelengths) if value in wv_opt]\n",
    "                # Count votes\n",
    "                wv_votes[wv_opt_ind] += 1\n",
    "            else:\n",
    "                print('NO ALL GREATER; BREAK')\n",
    "                print('--- End of optimization at Rep = ', rep, ' ---')\n",
    "                break\n",
    "\n",
    "# Store the votes\n",
    "all_wv_votes = wv_votes\n",
    "\n",
    "# Plot wavelength votes\n",
    "plot_wv_votes(col_wavelengths, wv_votes, i, k, save_flag=True)\n",
    "\n",
    "# End timestamp\n",
    "end_time = time.time()\n",
    "print('Time to run for each pair: ', (end_time - start_time) / 60, ' minutes')\n",
    "total_time.append(end_time - start_time)\n",
    "print('\\n')\n",
    "\n",
    "# Save variables\n",
    "dump(wv_votes, str(i) + '_' + str(k) + '_wv_votes.joblib')\n",
    "# Save variables\n",
    "#dump(all_wv_votes, 'all_wv_votes.joblib')\n",
    "dump(pls_rmsecv_baseline, str(i) + '_' + str(k) + '_pls_rmsecv_baseline.joblib')\n",
    "dump(all_avg_rmsecv, str(i) + '_' + str(k) + '_all_avg_rmsecv.joblib')\n",
    "dump(all_std_rmsecv, str(i) + '_' + str(k) + '_all_std_rmsecv.joblib')\n",
    "dump(all_min_rmsecv, str(i) + '_' + str(k) + '_all_min_rmsecv.joblib')\n",
    "dump(all_min_rmsecv_ind, str(i) + '_' + str(k) + '_all_min_rmsecv_ind.joblib')\n",
    "dump(total_time, str(i) + '_' + str(k) + '_total_time.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the highest vote and the final feature subset\n",
    "- This is for the OVR approach\n",
    "- Same applies to the OVO approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change to save_path\n",
    "os.chdir(save_path)\n",
    "os.chdir('YOUR SAVE PATH - SUBFOLDER')\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One vs Rest\n",
    "\n",
    "# Votes\n",
    "boneCement_rest_ovr_votes = load('YOUR VOTES.joblib')\n",
    "cortBone_rest_ovr_votes = load('YOUR VOTES.joblib')\n",
    "\n",
    "# X Train and Validation Dataset\n",
    "boneCement_X_train = load('YOUR TRAIN/TEST X SET.joblib')\n",
    "boneCement_y_train = load('YOUR TRAIN/TEST y SET.joblib')\n",
    "boneCement_X_val = load('YOUR VALIDATION X SET.joblib')\n",
    "boneCement_y_val = load('YOUR VALIDATION y SET.joblib')\n",
    "\n",
    "cortBone_X_train = load('YOUR TRAIN/TEST X SET.joblib')\n",
    "cortBone_y_train = load('YOUR TRAIN/TEST y SET.joblib')\n",
    "cortBone_X_val = load('YOUR VALIDATION X SET.joblib')\n",
    "cortBone_y_val = load('YOUR VALIDATION y SET.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "df_dataset = df_smooth\n",
    "tissue_types = tissue_types\n",
    "\n",
    "# Set global random state\n",
    "random_state = 42\n",
    "\n",
    "# Define cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=random_state)\n",
    "\n",
    "# Bone Cement = 1 vs. [Cortical Bone, Trabecular Bone, Cartilage, Bone Marrow] = 0\n",
    "i = 'boneCement'\n",
    "k_1 = 'cortBone'\n",
    "k_2 = 'traBone'\n",
    "k_3 = 'cartilage'\n",
    "k_4 = 'boneMarrow'\n",
    "\n",
    "# Cortical Bone = 1 vs. [Trabecular Bone, Cartilage, Bone Marrow, Muscle] = 0\n",
    "# i = 'cortBone'\n",
    "# k_1 = 'traBone'\n",
    "# k_2 = 'muscle'\n",
    "# k_3 = 'cartilage'\n",
    "# k_4 = 'boneMarrow'\n",
    "\n",
    "k = 'rest'\n",
    "print('First Label: ', i)\n",
    "#print('Second Label: ',k)\n",
    "print('Second Label: ' + k_1 + ', ' + k_2 + ', ' + k_3 + ', ' + k_4)\n",
    "\n",
    "# Data selection for multiple tissue types\n",
    "df_subset_ovr = df_dataset[df_dataset['target_y'].isin([i, k_1, k_2, k_3, k_4])]\n",
    "\n",
    "# Average spectra for each tissue type\n",
    "avg_df_select_tissues = df_subset_ovr.groupby(['target_y']).mean()\n",
    "stdev_df_select_tissues = df_subset_ovr.groupby(['target_y']).std()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set variables\n",
    "\n",
    "# Bone Cement\n",
    "votes_array = boneCement_rest_ovr_votes\n",
    "X_train = boneCement_X_train\n",
    "y_train = boneCement_y_train\n",
    "X_val = boneCement_X_val\n",
    "y_val = boneCement_y_val\n",
    "\n",
    "# Cortical Bone\n",
    "# votes_array = cortBone_rest_ovr_votes\n",
    "# X_train = cortBone_X_train\n",
    "# y_train = cortBone_y_train\n",
    "# X_val = cortBone_X_val\n",
    "# y_val = cortBone_y_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort in descending order\n",
    "votes_rank_idx = np.argsort(-votes_array)\n",
    "\n",
    "# Construct a dataframe\n",
    "votes_ranked_features = pd.DataFrame({'Features': col_wavelengths[votes_rank_idx],\n",
    "                                      'Votes': votes_array[votes_rank_idx],\n",
    "                                      'Ranking_idx': votes_rank_idx})\n",
    "\n",
    "# Select the highest VOTES\n",
    "select_top_votes = votes_ranked_features[votes_ranked_features[\"Votes\"] == max(votes_array)]\n",
    "\n",
    "# Selected X and y train\n",
    "X_train_select = X_train.iloc[:, select_top_votes[\"Ranking_idx\"].values]\n",
    "\n",
    "# Find the optimal number of PLS components out of 30\n",
    "pls_ncomp_opt = 30\n",
    "pls_ncomp_init, _, _, _ = plsr_ncomp(X_train_select, y_train, n_comp=pls_ncomp_opt, cv=10)\n",
    "pls_best_ncomp = pls_ncomp_init['n_components']\n",
    "print('PLS Number of Componenets: ', pls_best_ncomp)\n",
    "\n",
    "# Calculate PLS-VIP scores\n",
    "pls_vip = PLSRegression(n_components=pls_best_ncomp).fit(X_train_select, y_train)\n",
    "vip_coeffs = vip(pls_vip)\n",
    "\n",
    "# Selected wavelengths\n",
    "select_wv = X_train_select.columns.values.astype(np.float64)\n",
    "\n",
    "select_top_votes.insert(len(select_top_votes.columns), 'PLS-VIP', vip_coeffs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Tight layout\n",
    "f.tight_layout()\n",
    "\n",
    "# Plot\n",
    "plt.scatter(select_wv, vip_coeffs)\n",
    "ax.set_xlim([350, 1850])\n",
    "ax.set_title('PLS VIP Scores for Top Votes ' + str(i) + ' vs. ' + str(k))\n",
    "ax.set_xlabel('Wavelength (nm)')\n",
    "ax.set_ylabel('PLS VIP Scores (A.U.)')\n",
    "bbox = matplotlib.transforms.Bbox([[-0.45, -1.3], [11.45, 8.56]])\n",
    "\n",
    "if not os.path.exists(str(i) + '_' + str(k) + '_pls_vip.png'):\n",
    "    f.savefig(os.getcwd() + '/' +  str(i) + '_' + str(k) + '_pls_vip.png', dpi = 1080, bbox_inches =bbox)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the number of features for the final subset\n",
    "K_features = 10\n",
    "\n",
    "# Initiate the search\n",
    "selected_ind_update = [select_top_votes['Ranking_idx'][np.argmax(vip_coeffs)]]\n",
    "select_vip_ind_update = np.copy(select_top_votes['Ranking_idx'])\n",
    "select_vip_ind_update = np.delete(select_vip_ind_update, np.argmax(vip_coeffs))\n",
    "\n",
    "while len(selected_ind_update) < K_features:\n",
    "    #print('Number of Selected Ind: ', len(selected_ind_update))\n",
    "    pearson_rank = []\n",
    "    for n in range(len(select_vip_ind_update)):\n",
    "        pearson_corr = np.max(np.abs(np.corrcoef(X_train.iloc[:, selected_ind_update + [select_vip_ind_update[n]]], rowvar=False))\n",
    "                              - np.identity(len(selected_ind_update) + 1))\n",
    "        pearson_rank.append(pearson_corr)\n",
    "    selected_ind = np.argsort(pearson_rank)[0]\n",
    "    selected_select_vip_ind = select_vip_ind_update[selected_ind]\n",
    "    #print('Selected Ind: ', selected_anova_ind)\n",
    "    #print('Selected Pearson Coeff: ', pearson_rank[selected_ind])\n",
    "    selected_ind_update.append(selected_select_vip_ind)\n",
    "    select_vip_ind_update = np.delete(select_vip_ind_update, selected_ind)\n",
    "\n",
    "# Find the final selected wavelengths\n",
    "pls_vip_select_wv_final = col_wavelengths[selected_ind_update].values\n",
    "pls_vip_select_wv_final_str = [str(x) for x in pls_vip_select_wv_final]\n",
    "\n",
    "# Save\n",
    "dump(pls_vip_select_wv_final, str(i) + '_' + str(k) + '_pls_vip_select_wv_final.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "# Set the figure size\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Tight layout\n",
    "f.tight_layout()\n",
    "\n",
    "# Selected Tissue Types\n",
    "select_tissue_label = [i,k_1,k_2,k_3,k_4]\n",
    "select_tissue_label.sort()\n",
    "\n",
    "# Set custom color cycle\n",
    "if i == 'boneCement':\n",
    "    custom_cycler= (cycler('color', ['#d62728', '#808080', '#808080', '#808080', '#808080']) + cycler(lw=[1.5, 1.5, 1.5, 1.5, 1.5]))\n",
    "elif i == 'cortBone':\n",
    "    custom_cycler = (cycler('color', ['#808080', '#808080', '#d62728', '#808080', '#808080']) + cycler(lw=[1.5, 1.5, 1.5, 1.5, 1.5]))\n",
    "\n",
    "# Set colors\n",
    "ax.set_prop_cycle(custom_cycler)\n",
    "\n",
    "for tissue in range(avg_df_select_tissues.shape[0]):\n",
    "    plt.plot(col_wavelengths, avg_df_select_tissues.iloc[tissue, :], label=select_tissue_label[tissue])\n",
    "    pos_std = avg_df_select_tissues.iloc[tissue, :] + stdev_df_select_tissues.iloc[tissue, :]\n",
    "    neg_std = avg_df_select_tissues.iloc[tissue, :] - stdev_df_select_tissues.iloc[tissue, :]\n",
    "    ax.fill_between(col_wavelengths, neg_std, pos_std, alpha=0.08)\n",
    "\n",
    "for wv in pls_vip_select_wv_final:\n",
    "    plt.axvline(x=wv, color='g', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Text\n",
    "plt.text(0, -0.13, 'Selected Wavelengths - Final: ' + str(np.array(pls_vip_select_wv_final)), horizontalalignment='left',\n",
    "         verticalalignment = 'center_baseline', transform=ax.transAxes)\n",
    "\n",
    "# Set figure object\n",
    "ax.set_xlim([350, 1850])\n",
    "ax.set_title('Selected Wavelengths for ' + str(i) + ' vs. ' + str(k))\n",
    "ax.set_xlabel('Wavelength (nm)')\n",
    "ax.set_ylabel('Normalized Intensity (A.U.)')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "# Save\n",
    "bbox = matplotlib.transforms.Bbox([[-0.45, -1.3], [11.45, 8.56]])\n",
    "if not os.path.exists(str(i) + '_' + str(k) + 'pls_vip_select_wv_final_plot.png'):\n",
    "    f.savefig(os.getcwd() + '/' +  str(i) + '_' + str(k) + 'pls_vip_select_wv_final_plot.png', dpi = 1080, bbox_inches =bbox)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot Accuracy vs. Number of Features in the order given by the algorithm\n",
    "- or can rank the order by f_classif scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final Wavelengths\n",
    "\n",
    "boneCement_wv = load('YOUR PLS VIP SELECTED FEATURES.joblib')\n",
    "boneCement_wv = [str(x) for x in boneCement_wv]\n",
    "boneCement_wv = np.array(boneCement_wv)\n",
    "cortBone_wv = load('YOUR PLS VIP SELECTED FEATURES.joblib')\n",
    "cortBone_wv = [str(x) for x in cortBone_wv]\n",
    "cortBone_wv = np.array(cortBone_wv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bone Cement\n",
    "i = 'boneCement'\n",
    "k = 'rest'\n",
    "final_wv = boneCement_wv\n",
    "selected_X_train = X_train.loc[:, final_wv]\n",
    "selected_X_val = X_val.loc[:, final_wv]\n",
    "\n",
    "# Cortical Bone\n",
    "# i = 'cortBone'\n",
    "# k = 'rest'\n",
    "# final_wv = cortBone_wv\n",
    "# selected_X_train = X_train.loc[:, final_wv]\n",
    "# selected_X_val = X_val.loc[:, final_wv]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rank by f_classif\n",
    "\n",
    "# Select the top feature by ANOVA F-value\n",
    "fs = SelectKBest(f_classif,k=\"all\")\n",
    "fs.fit(selected_X_train,y_train)\n",
    "# Find the index from high to low\n",
    "f_idx = np.argsort(-fs.scores_)\n",
    "f_rank_wv = final_wv[f_idx]\n",
    "dump(f_rank_wv, str(i) + '_' + str(k) + '_pls_vip_wv_f_rank.joblib')\n",
    "\n",
    "# f ranked wavelengths\n",
    "selected_X_train = X_train.loc[:, f_rank_wv]\n",
    "selected_X_val = X_val.loc[:, f_rank_wv]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate clf accuracy\n",
    "\n",
    "# Define cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=random_state)\n",
    "\n",
    "all_lda_balanced_scores = []\n",
    "all_lda_balanced_stdev = []\n",
    "all_lda_balanced_scores_val = []\n",
    "all_lda_balanced_stdev_val = []\n",
    "\n",
    "for wv in range(len(final_wv)):\n",
    "    # Define LDA model\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "\n",
    "    # Calculate clf accuracy cv\n",
    "    print('Number of Features: ', wv+1)\n",
    "    _, lda_balanced_scores = clf_accuracy_eval(selected_X_train.iloc[:,0:wv+1], y_train, model, cv)\n",
    "    _, lda_balanced_scores_val = clf_accuracy_eval(selected_X_val.iloc[:,0:wv+1], y_val, model, cv)\n",
    "\n",
    "    all_lda_balanced_scores.append(np.mean(lda_balanced_scores))\n",
    "    all_lda_balanced_stdev.append(np.std(lda_balanced_scores))\n",
    "\n",
    "    all_lda_balanced_scores_val.append(np.mean(lda_balanced_scores_val))\n",
    "    all_lda_balanced_stdev_val.append(np.std(lda_balanced_scores_val))\n",
    "\n",
    "# Save Variables\n",
    "dump(all_lda_balanced_scores, str(i) + '_' + str(k) + '_lda_balanced_train_f_rank.joblib')\n",
    "dump(all_lda_balanced_stdev, str(i) + '_' + str(k) + '_lda_balanced_std_train_f_rank.joblib')\n",
    "\n",
    "dump(all_lda_balanced_scores_val, str(i) + '_' + str(k) + '_lda_balanced_val_f_rank.joblib')\n",
    "dump(all_lda_balanced_stdev_val, str(i) + '_' + str(k) + '_lda_balanced_std_val_f_rank.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot Clf Accuracy vs. Number of Features\n",
    "\n",
    "# Set the figure size\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "# Tight layout\n",
    "f.tight_layout()\n",
    "\n",
    "# Plot - Balanced accuracy on train\n",
    "plt.plot(np.linspace(1, len(f_rank_wv), len(f_rank_wv)), all_lda_balanced_scores, '--', lw=2.5)\n",
    "plt.errorbar(np.linspace(1, len(f_rank_wv), len(f_rank_wv)), all_lda_balanced_scores,\n",
    "             yerr=all_lda_balanced_stdev,\n",
    "             fmt='o', capsize=1.5)\n",
    "xlabels = np.append(np.array('0'), f_rank_wv)\n",
    "plt.xticks(np.arange(0, len(f_rank_wv) + 1, 1), xlabels)\n",
    "# Set figure object\n",
    "ax.set_title('Classification Accuracy vs. Number of Wavelengths for ' + str(i) + ' vs. ' + str(k))\n",
    "ax.set_xlabel('Wavelengths Included (nm)')\n",
    "ax.set_ylabel('LDA Balanced Clf Accuracy')\n",
    "#ax.set_xlim([350, 1850])\n",
    "bbox = matplotlib.transforms.Bbox([[-0.4, -0.4], [8.5, 6.5]])\n",
    "\n",
    "# Save figure\n",
    "if not os.path.exists(str(i) + '_' + str(k) + '_balanced_nFeatures_train_f_rank.png'):\n",
    "    f.savefig(os.getcwd() + '/' + str(i) + '_' + str(k) + '_balanced_nFeatures_train_f_rank.png', dpi=1080,\n",
    "              bbox_inches=bbox)\n",
    "\n",
    "\n",
    "# plot Clf Accuracy vs. Number of Features\n",
    "\n",
    "# Set the figure size\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "# Tight layout\n",
    "f.tight_layout()\n",
    "\n",
    "# Plot - Balanced accuracy on validation\n",
    "plt.plot(np.linspace(1, len(f_rank_wv), len(f_rank_wv)), all_lda_balanced_scores_val, '--', lw=2.5)\n",
    "plt.errorbar(np.linspace(1, len(f_rank_wv), len(f_rank_wv)), all_lda_balanced_scores_val,\n",
    "             yerr=all_lda_balanced_stdev_val,\n",
    "             fmt='o', capsize=1.5)\n",
    "xlabels = np.append(np.array('0'), f_rank_wv)\n",
    "plt.xticks(np.arange(0, len(f_rank_wv) + 1, 1), xlabels)\n",
    "# Set figure object\n",
    "ax.set_title('Classification Accuracy vs. Number of Wavelengths for ' + str(i) + ' vs. ' + str(k))\n",
    "ax.set_xlabel('Wavelengths Included (nm)')\n",
    "ax.set_ylabel('LDA Balanced Clf Accuracy')\n",
    "#ax.set_xlim([350, 1850])\n",
    "bbox = matplotlib.transforms.Bbox([[-0.4, -0.4], [8.5, 6.5]])\n",
    "\n",
    "# Save figure\n",
    "if not os.path.exists(str(i) + '_' + str(k) + '_balanced_nFeatures_val_f_rank.png'):\n",
    "    f.savefig(os.getcwd() + '/' + str(i) + '_' + str(k) + '_balanced_nFeatures_val_f_rank.png', dpi=1080, bbox_inches=bbox)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}